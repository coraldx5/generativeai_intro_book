{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJ/tRCTJZesvcOvEeD3w0f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coraldx5/generativeai_intro_book/blob/master/chap05_LLM_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLM（Masked Language Model）"
      ],
      "metadata": {
        "id": "q4l2Vi7cDL9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fugashi ipadic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMlwu6MTDg4Y",
        "outputId": "b7c9f051-b83e-4f13-9ddb-80997c480335"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fugashi in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Collecting ipadic\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ipadic\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556704 sha256=942c24138157ba8386145b270f617186b68f2984e47a0d32146f53b41fd85bcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/ea/e3/2f6e0860a327daba3b030853fce4483ed37468bbf1101c59c3\n",
            "Successfully built ipadic\n",
            "Installing collected packages: ipadic\n",
            "Successfully installed ipadic-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM, pipeline\n",
        "\n",
        "# トークナイザと訓練済みモデルの読み込み\n",
        "# 'cl-tohoku/bert-base-japanese-whole-word-masking' という事前学習済みの日本語BERTモデルを使用します。\n",
        "model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
        "\n",
        "# 事前学習済みモデルに対応するトークナイザーをロードします。\n",
        "# BertJapaneseTokenizerはテキストをトークン（モデルが理解できる単位）に変換し、逆にトークンからテキストに変換する役割を持ちます。\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
        "\n",
        "# パイプラインの定義\n",
        "# 'fill-mask' タスクのパイプラインを作成します。これは文章の中の [MASK] トークンを予測するためのものです。\n",
        "fill_mask = pipeline('fill-mask',\n",
        "                     model=model,        # 言語モデルの指定\n",
        "                     tokenizer=tokenizer, # トークナイザの指定\n",
        "                     top_k=6              # 表示する候補数の指定\n",
        "                    )\n",
        "\n",
        "# 結果を表示する関数の定義\n",
        "# 文章を入力として、[MASK] トークンの候補とその確率を表示する関数を定義します。\n",
        "def predictmask(text):\n",
        "    print('---' * 10)\n",
        "    print(f'元の文章：「{text}」')\n",
        "    print(f'[MASK]部の候補：')\n",
        "    for res in fill_mask(text):\n",
        "        print(f\"{res['score']:.4f}: {res['token_str']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qONEY1D6bZa",
        "outputId": "0be50953-e7af-4c27-9d5a-55c4f2215110"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictmask('サングラスをかけた[MASK]が公園を駆け回る。')\n",
        "predictmask('サングラスをかけた[MASK]を食べるのが楽しみだ')\n",
        "predictmask('生卵をかけた[MASK]を食べるのが楽しみだ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sswxk076gC5",
        "outputId": "08000b40-9fde-4bd9-964e-3d3b1e35d713"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "元の文章：「サングラスをかけた[MASK]が公園を駆け回る。」\n",
            "[MASK]部の候補：\n",
            "0.2395: 男\n",
            "0.0848: 少年\n",
            "0.0511: 少女\n",
            "0.0433: 犬\n",
            "0.0331: 青年\n",
            "0.0293: 女性\n",
            "------------------------------\n",
            "元の文章：「サングラスをかけた[MASK]を食べるのが楽しみだ」\n",
            "[MASK]部の候補：\n",
            "0.0882: もの\n",
            "0.0608: ケーキ\n",
            "0.0493: 料理\n",
            "0.0314: 朝食\n",
            "0.0312: 犬\n",
            "0.0305: ご飯\n",
            "------------------------------\n",
            "元の文章：「生卵をかけた[MASK]を食べるのが楽しみだ」\n",
            "[MASK]部の候補：\n",
            "0.1659: もの\n",
            "0.1010: ご飯\n",
            "0.0815: 料理\n",
            "0.0752: スープ\n",
            "0.0658: 卵\n",
            "0.0490: パン\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## UIの参考：穴埋め問題を解くサンプルコード\n",
        "# @markdown 入力欄に、[MASK]を含む文章を入力してください。\n",
        "check_sentence = \"サングラスをかけた[MASK]が公園を駆け回る。\" # @param {type:\"string\"}\n",
        "predictmask(check_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "B-7KuJ3RGmkO",
        "outputId": "1c8b6bbd-e0ba-4893-e2a4-bf95d409f07d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "元の文章：「サングラスをかけた[MASK]が公園を駆け回る。」\n",
            "[MASK]部の候補：\n",
            "0.2395: 男\n",
            "0.0848: 少年\n",
            "0.0511: 少女\n",
            "0.0433: 犬\n",
            "0.0331: 青年\n",
            "0.0293: 女性\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLM（Causal Language Model）"
      ],
      "metadata": {
        "id": "aL_wYL7vEABr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import GPT2LMHeadModel, T5Tokenizer\n",
        "\n",
        "# モデルとトークナイザーのロード\n",
        "# 'rinna/japanese-gpt2-medium' という事前学習済みの日本語GPT-2モデルを使用します。モデルのサイズは約1.37GBです。\n",
        "model_name = 'rinna/japanese-gpt2-medium'  # モデルの名前を指定\n",
        "\n",
        "# 事前学習済みのGPT-2モデルをロードします。GPT2LMHeadModelはテキスト生成タスクに使用されるモデルです。\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# 事前学習済みモデルに対応するトークナイザーをロードします。\n",
        "# T5Tokenizerはテキストをトークン（モデルが理解できる単位）に変換し、逆にトークンからテキストに変換する役割を持ちます。\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate_text(input_text, max_length):\n",
        "    # 入力文章をトークン化\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # Attention maskの設定\n",
        "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
        "\n",
        "    # テキスト生成\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,  # Attention maskを指定\n",
        "        max_length=max_length,  # 生成する最大トークン数\n",
        "        # no_repeat_ngram_size=2,  # 繰り返しを防ぐn-gramのサイズ\n",
        "        pad_token_id=tokenizer.pad_token_id,  # パディングのトークンID\n",
        "        bos_token_id=tokenizer.bos_token_id,  # テキスト先頭のトークンID\n",
        "        eos_token_id=tokenizer.eos_token_id,  # テキスト終端のトークンID\n",
        "    )\n",
        "\n",
        "    # 生成されたテキストのデコード\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "VJUG0kK-cMsT"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 関数の使用例\n",
        "input_text = \"これから雨が降りそうなので、\"  # 入力テキスト\n",
        "max_length = 50  # 生成する最大トークン数\n",
        "\n",
        "# 生成されたテキストを表示\n",
        "generated_text = generate_text(input_text, max_length)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOJTPWBxEHpu",
        "outputId": "8beb673e-fbe2-4144-e020-554ff96e016e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "これから雨が降りそうなので、今日は、お休みです。 明日は、お休みです。 今日は、お休みです。 今日は、お休みです。 今日は、お休みです。 今日\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## UIの参考：後続文章を生成するサンプルコード\n",
        "# @markdown 文章を入力して下さい\n",
        "input_text = \"これから雨が降りそうなので、\" # @param {type:\"string\"}\n",
        "# @markdown 生成する最大トークン数\n",
        "max_length = 50 # @param {type:\"integer\"}\n",
        "generate_text(input_text, max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NRHw-qPkG4UU",
        "outputId": "773e7efe-79ed-4c06-c49d-062531409b84"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'これから雨が降りそうなので、今日は、お休みです。 明日は、お休みです。 今日は、お休みです。 今日は、お休みです。 今日は、お休みです。 今日'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 各トークンの予測確率を可視化してみよう"
      ],
      "metadata": {
        "id": "t3Zo7YfeXMZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルを評価モードに設定\n",
        "model.eval()\n",
        "\n",
        "# 入力文章をトークン化\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# Attention maskの設定\n",
        "attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
        "\n",
        "# 生成されたトークンとその確率を順次表示\n",
        "max_length = 10  # 生成する最大トークン数\n",
        "for _ in range(max_length):\n",
        "    # トークンの予測確率を取得\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        predictions = outputs.logits\n",
        "\n",
        "    # 次のトークンの予測確率を計算\n",
        "    next_token_probs = torch.softmax(predictions[:, -1, :], dim=-1)\n",
        "\n",
        "    # 上位3つのトークンを取得\n",
        "    top_k = 3\n",
        "    top_k_probs, top_k_indices = torch.topk(next_token_probs, top_k)\n",
        "\n",
        "    # 上位3つのトークンとその確率を表示\n",
        "    print(f\"\\n({_+1}番目) 上位3つのトークンと、確率：\")\n",
        "    for i in range(top_k):\n",
        "        predicted_token_id = top_k_indices[0, i].item()\n",
        "        predicted_token = tokenizer.decode([predicted_token_id])\n",
        "        predicted_prob = top_k_probs[0, i].item()\n",
        "        print(f\"Token: {predicted_token}({predicted_token_id}), Probability: {predicted_prob:.4f}\")\n",
        "\n",
        "    # 最も確率の高いトークンを入力トークンに追加\n",
        "    input_ids = torch.cat((input_ids, top_k_indices[:, 0].unsqueeze(-1)), dim=1)\n",
        "\n",
        "    # 予測が終了トークンに到達した場合は終了\n",
        "    if top_k_indices[0, 0].item() == tokenizer.eos_token_id:\n",
        "        break\n",
        "\n",
        "# 生成された全テキストを表示\n",
        "generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "print(\"---\"*10)\n",
        "print(\"生成された文章:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCrv7XGhguHw",
        "outputId": "3abff3b7-48f2-4d5a-bca8-7a1f5d8461fa"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(1番目) 上位3つのトークンと、確率：\n",
            "Token: 今日(4761), Probability: 0.0668\n",
            "Token: 雨(1537), Probability: 0.0178\n",
            "Token: 早(1745), Probability: 0.0128\n",
            "\n",
            "(2番目) 上位3つのトークンと、確率：\n",
            "Token: は(11), Probability: 0.7620\n",
            "Token: も(30), Probability: 0.0637\n",
            "Token: はこの(1007), Probability: 0.0170\n",
            "\n",
            "(3番目) 上位3つのトークンと、確率：\n",
            "Token: 、(7), Probability: 0.1085\n",
            "Token: (9), Probability: 0.0632\n",
            "Token: お(220), Probability: 0.0614\n",
            "\n",
            "(4番目) 上位3つのトークンと、確率：\n",
            "Token: お(220), Probability: 0.0566\n",
            "Token: (9), Probability: 0.0373\n",
            "Token: いつも(9328), Probability: 0.0168\n",
            "\n",
            "(5番目) 上位3つのトークンと、確率：\n",
            "Token: 休み(19138), Probability: 0.1112\n",
            "Token: 昼(5766), Probability: 0.0946\n",
            "Token: 散歩(22925), Probability: 0.0602\n",
            "\n",
            "(6番目) 上位3つのトークンと、確率：\n",
            "Token: です(2767), Probability: 0.1763\n",
            "Token: を(18), Probability: 0.1137\n",
            "Token: に(17), Probability: 0.0931\n",
            "\n",
            "(7番目) 上位3つのトークンと、確率：\n",
            "Token: 。(8), Probability: 0.6789\n",
            "Token: (9), Probability: 0.0664\n",
            "Token: ((15), Probability: 0.0511\n",
            "\n",
            "(8番目) 上位3つのトークンと、確率：\n",
            "Token: (9), Probability: 0.6433\n",
            "Token: お(14287), Probability: 0.0382\n",
            "Token: 。(8), Probability: 0.0373\n",
            "\n",
            "(9番目) 上位3つのトークンと、確率：\n",
            "Token: 明日(14787), Probability: 0.0737\n",
            "Token: 今日(4761), Probability: 0.0680\n",
            "Token: 昨(12096), Probability: 0.0478\n",
            "\n",
            "(10番目) 上位3つのトークンと、確率：\n",
            "Token: は(11), Probability: 0.4946\n",
            "Token: 、(7), Probability: 0.1335\n",
            "Token: も(30), Probability: 0.1334\n",
            "------------------------------\n",
            "生成された文章:\n",
            "これから雨が降りそうなので、今日は、お休みです。 明日は\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, T5Tokenizer\n",
        "\n",
        "# モデルとトークナイザーのロード\n",
        "model_name = 'rinna/japanese-gpt2-medium'\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# テキスト生成の設定\n",
        "input_text = \"これから雨が降りそうなので、\"  # 入力テキスト\n",
        "max_length = 40  # 生成する最大トークン数\n",
        "\n",
        "# トークン化\n",
        "# 入力テキストをモデルが理解できるトークンに変換します\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# テキスト生成のパラメータを辞書で設定\n",
        "# max_length: 生成する最大トークン数\n",
        "# pad_token_id: パディングのトークンID\n",
        "# bos_token_id: テキスト先頭のトークンID\n",
        "# eos_token_id: テキスト終端のトークンID\n",
        "prm = {\n",
        "    \"max_length\": max_length,\n",
        "    \"pad_token_id\": tokenizer.pad_token_id,\n",
        "    \"bos_token_id\": tokenizer.bos_token_id,\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,\n",
        "}\n",
        "\n",
        "# Greedy探索\n",
        "# Greedy探索は、各ステップで最も確率の高いトークンを選びます\n",
        "greedy_output = model.generate(input_ids, **prm)\n",
        "print(\"Greedy:\", tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
        "\n",
        "# Beam探索\n",
        "# Beam探索は、複数の候補（ビーム）を同時に探索し、最も良い結果を選びます\n",
        "# num_beams: ビームの数\n",
        "# early_stopping: 生成の早期終了を行うかどうか\n",
        "beam_output = model.generate(input_ids, num_beams=3, early_stopping=True, **prm)\n",
        "print(\"Beam:\", tokenizer.decode(beam_output[0], skip_special_tokens=True))\n",
        "\n",
        "# Top-kサンプリング\n",
        "# Top-kサンプリングは、上位k個のトークンからランダムに選択します\n",
        "# do_sample: サンプリングを行うかどうか\n",
        "# top_k: 選択する上位トークンの数\n",
        "top_k_output = model.generate(input_ids, do_sample=True, top_k=50, **prm)\n",
        "print(\"Top-k Sampling:\", tokenizer.decode(top_k_output[0], skip_special_tokens=True))\n",
        "\n",
        "# Top-pサンプリング\n",
        "# Top-pサンプリングは、確率の高いトークンの集合からランダムに選択します\n",
        "# do_sample: サンプリングを行うかどうか\n",
        "# top_p: 累積確率がpを超えるまでのトークンを選択する閾値\n",
        "top_p_output = model.generate(input_ids, do_sample=True, top_p=0.95, **prm)\n",
        "print(\"Top-p Sampling:\", tokenizer.decode(top_p_output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avMUcVhTa-kr",
        "outputId": "31188e06-a553-499d-9c2b-3fd5f819c45a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy: これから雨が降りそうなので、今日は、お休みです。 明日は、お休みです。 今日は、お休みです。 今日は、お休みです。\n",
            "Beam: これから雨が降りそうなので、早めに切り上げました。 今日は、朝から雨が降っていましたが、 午後からは晴れてきました。 今日は\n",
            "Top-k Sampling: これから雨が降りそうなので、近所の公園まで自転車を走らせることに。 先日お弁当食べてたときに、このことを言われました。 いやぁ～\n",
            "Top-p Sampling: これから雨が降りそうなので、自転車に乗っている方には 傘をお願いします。 昨日は、お見舞いの品を届けに行ってきました。。。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## UIの参考：様々な探索法で文章を生成するサンプルコード\n",
        "# @markdown 文章を入力して下さい\n",
        "# テキスト生成の設定\n",
        "input_text = \"これから雨が降りそうなので、\" # @param {type:\"string\"}\n",
        "# @markdown 生成する最大トークン数\n",
        "max_length = 50 # @param {type:\"integer\"}\n",
        "\n",
        "# トークン化\n",
        "# 入力テキストをモデルが理解できるトークンに変換します\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# テキスト生成のパラメータを辞書で設定\n",
        "# max_length: 生成する最大トークン数\n",
        "# pad_token_id: パディングのトークンID\n",
        "# bos_token_id: テキスト先頭のトークンID\n",
        "# eos_token_id: テキスト終端のトークンID\n",
        "prm = {\n",
        "    \"max_length\": max_length,\n",
        "    \"pad_token_id\": tokenizer.pad_token_id,\n",
        "    \"bos_token_id\": tokenizer.bos_token_id,\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,\n",
        "}\n",
        "\n",
        "# Greedy探索\n",
        "# Greedy探索は、各ステップで最も確率の高いトークンを選びます\n",
        "greedy_output = model.generate(input_ids, **prm)\n",
        "print(\"Greedy:\", tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
        "\n",
        "# Beam探索\n",
        "# Beam探索は、複数の候補（ビーム）を同時に探索し、最も良い結果を選びます\n",
        "# num_beams: ビームの数\n",
        "# early_stopping: 生成の早期終了を行うかどうか\n",
        "beam_output = model.generate(input_ids, num_beams=3, early_stopping=True, **prm)\n",
        "print(\"Beam:\", tokenizer.decode(beam_output[0], skip_special_tokens=True))\n",
        "\n",
        "# Top-kサンプリング\n",
        "# Top-kサンプリングは、上位k個のトークンからランダムに選択します\n",
        "# do_sample: サンプリングを行うかどうか\n",
        "# top_k: 選択する上位トークンの数\n",
        "top_k_output = model.generate(input_ids, do_sample=True, top_k=50, **prm)\n",
        "print(\"Top-k Sampling:\", tokenizer.decode(top_k_output[0], skip_special_tokens=True))\n",
        "\n",
        "# Top-pサンプリング\n",
        "# Top-pサンプリングは、確率の高いトークンの集合からランダムに選択します\n",
        "# do_sample: サンプリングを行うかどうか\n",
        "# top_p: 累積確率がpを超えるまでのトークンを選択する閾値\n",
        "top_p_output = model.generate(input_ids, do_sample=True, top_p=0.95, **prm)\n",
        "print(\"Top-p Sampling:\", tokenizer.decode(top_p_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jJuq9RYdksyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# worv2vec\n",
        "- [東北大学のページ](https://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/)から学習済モデル(20170201.tar.bz2)をダウンロードして使用します。\n",
        "- 同ホームページからの抜粋：\n",
        "  - 「日本語 Wikipedia エンティティベクトル」は、[日本語版 Wikipedia](https://ja.wikipedia.org/wiki/) の本文全文から学習した、単語、および Wikipedia で記事となっているエンティティの分散表現ベクトルです。Wikipedia の記事本文の抽出には [WikiExtractor ](https://github.com/attardi/wikiextractor)を、単語分割には [MeCab](http://taku910.github.io/mecab/) を、単語ベクトルの学習には [word2vec](https://code.google.com/archive/p/word2vec/\n",
        ") をそれぞれ用いています。"
      ],
      "metadata": {
        "id": "-TAk1uhBi0gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/data/20170201.tar.bz2\n",
        "!tar xf 20170201.tar.bz2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39aMZyVIjWPm",
        "outputId": "91d04702-4d49-40da-a5a3-259f82a998b9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-27 14:56:50--  https://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/data/20170201.tar.bz2\n",
            "Resolving www.cl.ecei.tohoku.ac.jp (www.cl.ecei.tohoku.ac.jp)... 130.34.192.83\n",
            "Connecting to www.cl.ecei.tohoku.ac.jp (www.cl.ecei.tohoku.ac.jp)|130.34.192.83|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1373795477 (1.3G) [application/x-bzip2]\n",
            "Saving to: ‘20170201.tar.bz2’\n",
            "\n",
            "20170201.tar.bz2    100%[===================>]   1.28G  11.2MB/s    in 2m 8s   \n",
            "\n",
            "2024-05-27 14:58:59 (10.2 MB/s) - ‘20170201.tar.bz2’ saved [1373795477/1373795477]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "output_file は結果を書き込むファイルを開くための変数です。result.txt という名前のファイルに書き込みます。\n",
        "display_and_save_list 関数は、リスト内の単語とその類似度を表示し、ファイルに書き込むための関数です。\n",
        "word2vec_model は、事前に訓練されたWord2Vecモデルをロードするための変数です。\n",
        "similar_words_mickey は、ミッキーと類似度が高い単語のリストを取得します。\n",
        "similar_words_mickey_christmas は、ミッキーとクリスマスの両方に関連する単語のリストを取得します。\n",
        "similar_words_usj は、ミッキーからディズニーを引き、ユニバーサルスタジオジャパンを足した場合に関連する単語のリストを取得します。"
      ],
      "metadata": {
        "id": "9pEYyvwEnh8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# 結果を保存するファイルを開く\n",
        "output_file = codecs.open('result.txt', 'w', 'utf-8')\n",
        "\n",
        "# リストを表示およびファイルに書き込む関数\n",
        "def display_and_save_list(word_list):\n",
        "    for word, similarity in word_list:\n",
        "        # 表示\n",
        "        print(word, str(similarity))\n",
        "        # ファイルに書き込み\n",
        "        output_file.write(f\"{word},{similarity}\\n\")\n",
        "    print(\"---\")\n",
        "\n",
        "# モデルをロード\n",
        "word2vec_model = KeyedVectors.load_word2vec_format('./entity_vector/entity_vector.model.bin', binary=True)\n",
        "\n",
        "# 王様と類似度が高い言葉を取得\n",
        "similar_words_mickey = word2vec_model.most_similar('王様')\n",
        "display_and_save_list(similar_words_mickey)\n",
        "\n",
        "# 王様とブルボン朝と類似の言葉を取得（王様 + ブルボン朝）\n",
        "similar_words_mickey_christmas = word2vec_model.most_similar(positive=['王様', 'ブルボン朝'])\n",
        "display_and_save_list(similar_words_mickey_christmas)\n",
        "\n",
        "# ユニバーサルスタジオ界のミッキーは誰か（ミッキー - ディズニー + ユニバーサルスタジオジャパン）\n",
        "similar_words_usj = word2vec_model.most_similar(positive=['王様', '江戸時代'], negative=['ブルボン朝'])\n",
        "display_and_save_list(similar_words_usj)\n"
      ],
      "metadata": {
        "id": "BS3u_jyldRgR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a805dd4e-ee7d-4205-bc11-ad4cf554a88d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "お姫様 0.7911006808280945\n",
            "おじいさん 0.708034098148346\n",
            "[シンデレラ] 0.7062105536460876\n",
            "[アリス_(不思議の国のアリス)] 0.7054476737976074\n",
            "魔法使い 0.6919453740119934\n",
            "妖精 0.6900907754898071\n",
            "貴婦人 0.6897401809692383\n",
            "パパ 0.6879994869232178\n",
            "[白雪姫] 0.6826063990592957\n",
            "魔女 0.6819620132446289\n",
            "---\n",
            "エツィオ 0.6156821250915527\n",
            "ディド 0.6051962971687317\n",
            "シッド 0.6026549339294434\n",
            "[フロージ] 0.5943021178245544\n",
            "王笏 0.5941326022148132\n",
            "エイゴン 0.5928665995597839\n",
            "[ロスタム] 0.5919443368911743\n",
            "[ウォート] 0.5914792418479919\n",
            "ハーコン 0.5900202393531799\n",
            "リヴァ 0.588994562625885\n",
            "---\n",
            "殿様 0.6322947144508362\n",
            "芸者 0.5856623649597168\n",
            "神様 0.5821214318275452\n",
            "老婆 0.5772008895874023\n",
            "遊女 0.5680871605873108\n",
            "狸 0.5647869110107422\n",
            "[遊女] 0.5636616945266724\n",
            "御馳走 0.5611592531204224\n",
            "[花魁] 0.560475766658783\n",
            "女房 0.5492725372314453\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルのパラメータ数を確認\n",
        "vocab_size = len(word2vec_model.key_to_index)  # 語彙サイズ\n",
        "vector_size = word2vec_model.vector_size       # 各ベクトルの次元数\n",
        "total_parameters = vocab_size * vector_size    # パラメータの総数\n",
        "\n",
        "print(f\"語彙サイズ: {vocab_size}\")\n",
        "print(f\"ベクトル次元数: {vector_size}\")\n",
        "print(f\"パラメータの総数: {total_parameters}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYTJbgU_oCt_",
        "outputId": "72b8cee7-0060-492f-cf5d-d7c1d3f16b1a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "語彙サイズ: 1015474\n",
            "ベクトル次元数: 200\n",
            "パラメータの総数: 203094800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CQJy5vAGr07U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}